{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjW7LukyVdnW"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzjgvXr0F1Cg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "  if not os.path.isdir('pa_county'):\n",
        "    !unzip pa_County.zip -d pa_county\n",
        "  pass\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  import geopandas as gpd\n",
        "except:\n",
        "  !pip install geopandas\n",
        "  import geopandas as gpd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "fp = \"pa_county/PaCounty2024_03.shp\"\n",
        "\n",
        "#reading the file stored in variable fp\n",
        "map_df = gpd.read_file(fp)\n",
        "map_df = map_df.to_crs(epsg=4326)\n",
        "map_df.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTj8SRtE0AMt"
      },
      "outputs": [],
      "source": [
        "#sort according to fips\n",
        "#map_df = map_df.sort_values('GEOID')\n",
        "\n",
        "def swap_columns(df, col1, col2):\n",
        "    col_list = list(df.columns)\n",
        "    x, y = col_list.index(col1), col_list.index(col2)\n",
        "    col_list[y], col_list[x] = col_list[x], col_list[y]\n",
        "    df = df[col_list]\n",
        "    return df\n",
        "\n",
        "#to make Allegheny county the first row as it's in the data\n",
        "def swap_rows(df, i1, i2):\n",
        "    a, b = df.iloc[i1, :].copy(), df.iloc[i2, :].copy()\n",
        "    df.iloc[i1, :], df.iloc[i2, :] = b, a\n",
        "    return df\n",
        "\n",
        "map_df = map_df.sort_values('FIPS_COUNT')\n",
        "map_df = map_df.reset_index(drop=True)\n",
        "#map_df = swap_columns(map_df, 'FIPS_COUNT', 'GEOID')\n",
        "\n",
        "# 1 in case we're using oud_pa2.csv\n",
        "oud_file = 1\n",
        "if oud_file == 1:\n",
        "  map_df = swap_rows(map_df, 0, 1)\n",
        "map_df = map_df.reset_index(drop=True)\n",
        "map_df = map_df.rename(columns={'FIPS_COUNT': 'FIPS'})\n",
        "map_df = map_df.rename(columns={'COUNTY_NAM': 'NAME'})\n",
        "map_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klHWvGpoL1Q9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "df = [0]*25\n",
        "print(df)\n",
        "\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    df_temp = pd.read_csv('data_pa_int_'+str(i+1)+str(j+1)+'.csv')\n",
        "    df_temp = df_temp.dropna()\n",
        "    df_temp.columns = map_df['NAME'].tolist()\n",
        "    df[i*5+j] = df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-f8vAIr3ZM2"
      },
      "outputs": [],
      "source": [
        "df_pop = pd.read_csv('pop_pa.csv')\n",
        "df_pop = df_pop.dropna()\n",
        "df_pop = df_pop.drop_duplicates(subset='County Name', keep=\"last\")\n",
        "delete_row = df_pop[df_pop['County Name'] == 'Pennsylvania'].index\n",
        "df_pop = df_pop.drop(delete_row)\n",
        "df_pop = df_pop.sort_values('County Name')\n",
        "df_pop = swap_rows(df_pop, 0, 1)\n",
        "df_pop = df_pop.reset_index(drop=True)\n",
        "\n",
        "area = map_df['AREA_SQ_MI'].tolist()\n",
        "pop = df_pop['Population'].tolist()\n",
        "pop_den = pop/np.array(area)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHf_n1e5AG23"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "df_income = pd.read_csv('house_income.csv')\n",
        "df_income = df_income.dropna()\n",
        "\n",
        "df_income = swap_rows(df_income, 0, 1)\n",
        "df_income = df_income.reset_index(drop=True)\n",
        "\n",
        "income = df_income['Income'].tolist()\n",
        "income = [int(i/1000) for i in income]\n",
        "\n",
        "df_black_percent = pd.read_csv('pa_black_percent.csv')\n",
        "df_black_percent = df_black_percent.dropna()\n",
        "\n",
        "black_p = df_black_percent['Percent'].tolist()\n",
        "\n",
        "df_rurality = pd.read_csv('rurality.csv')\n",
        "df_rurality = df_rurality.dropna()\n",
        "\n",
        "rurality = df_rurality['Index'].tolist()\n",
        "\n",
        "df_unemploy = pd.read_csv('unemploy_pa.csv')\n",
        "df_unemploy = df_unemploy.dropna()\n",
        "unemploy = df_unemploy['rate'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Keua9R9k_1Ji"
      },
      "outputs": [],
      "source": [
        "map_df['FIPS'] = ('42'+map_df['FIPS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGk_RjUKYSaN"
      },
      "source": [
        "### Indicate which files to use i.e., counts or rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb8E0Wochzcf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "change_file = 0\n",
        "if change_file == 1:\n",
        "  df = df_abs\n",
        "  df_int2 = df_abs_int2\n",
        "  df_int3 = df_abs_int3\n",
        "  df_int4 = df_abs_int4\n",
        "  df_int5 = df_abs_int5\n",
        "\n",
        "  df_average = [0]*len(map_df)\n",
        "  df_std = [0]*len(map_df)\n",
        "  for i in range(len(df.iloc[0])-2):\n",
        "    ls = []\n",
        "    for j in range(len(df)-1):\n",
        "      #print(i,j,df.iloc[j][i])\n",
        "      ls.append(int(df.iloc[j][i]))\n",
        "\n",
        "    df_average[i] = sum(ls) / len(ls)\n",
        "    df_std[i] = np.std(ls)\n",
        "\n",
        "'''\n",
        "change_file = 0\n",
        "if change_file == 1:\n",
        "  for i in range(5):\n",
        "    for j in range(5):\n",
        "      df[i*5+j] =  np.log(df[i*5+j]/np.array(pop)*100000 + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa2vHwz93LmO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "arr = [float(df[0][map_df['NAME'].loc[i]].loc[i]) for i in range(len(map_df))]\n",
        "merged = map_df.join(pd.DataFrame(arr,columns=['ODD']))\n",
        "\n",
        "merged.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8-KP7hlVH18"
      },
      "source": [
        "\n",
        "\n",
        "# GPR related functions\n",
        "### Loading required libraries and creating essential functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d4pH6HopQ0uG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "try:\n",
        "  from botorch.models import SingleTaskGP, Ex, MultiTaskGP\n",
        "except:\n",
        "  !pip install botorch\n",
        "  from botorch.models import SingleTaskGP, MultiTaskGP\n",
        "\n",
        "from botorch.fit import fit_gpytorch_mll\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood, MarginalLogLikelihood\n",
        "from botorch.models.transforms.outcome import Standardize\n",
        "\n",
        "from gpytorch.kernels import MaternKernel, RBFKernel, LinearKernel, RQKernel, SpectralDeltaKernel, SpectralMixtureKernel, ScaleKernel, IndexKernel, PeriodicKernel\n",
        "\n",
        "\n",
        "from scipy.stats import qmc\n",
        "sampler = qmc.LatinHypercube(d=2)\n",
        "sample = sampler.random(n=1)\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "ls = np.array([map_df['geometry'].loc[0].centroid.x, map_df['geometry'].loc[0].centroid.y,\n",
        "               #income[0],\n",
        "               pop_den[0], #black_p[0],\n",
        "               #native_p[0], #poverty[0]\n",
        "               ]).reshape(1,-1)\n",
        "y = np.array(float(df[0][map_df['NAME'].iloc[0]].iloc[0])).reshape(-1,1)\n",
        "\n",
        "for i in range(1,len(map_df)):\n",
        "  #print(i)\n",
        "  for j in range(batch_size):\n",
        "    ls = np.append(ls, np.array([map_df['geometry'].loc[i].centroid.x, map_df['geometry'].loc[i].centroid.y,\n",
        "                                 #income[i],\n",
        "                                 pop_den[i], #black_p[i], #native_p[i]\n",
        "                                 #native_p[i], #poverty[i]\n",
        "                                 ]).reshape(1,-1), axis=0)\n",
        "    y = np.append(y, np.array(float(df[0][map_df['NAME'].iloc[i]].iloc[j+1])).reshape(-1,1), axis=0)\n",
        "\n",
        "#train_X = torch.rand(10, 2, dtype=torch.float64)\n",
        "train_X = torch.Tensor(np.array(ls)).double()\n",
        "train_Y = torch.Tensor(y, ).double()\n",
        "\n",
        "part = [ls[:,i] for i in range(np.shape(ls)[1])]\n",
        "\n",
        "choice = torch.Tensor(np.array([i for i in part]))\n",
        "#print(choice, choice.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDs48P-LVRnc"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition import UpperConfidenceBound\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement, qNoisyExpectedImprovement\n",
        "from botorch.acquisition.logei import qLogExpectedImprovement\n",
        "from botorch.acquisition.active_learning import qNegIntegratedPosteriorVariance\n",
        "from botorch.acquisition.analytic import PosteriorStandardDeviation\n",
        "\n",
        "from botorch.acquisition.max_value_entropy_search import qMaxValueEntropy\n",
        "from botorch.acquisition.predictive_entropy_search import qPredictiveEntropySearch\n",
        "\n",
        "from botorch.optim import optimize_acqf, optimize_acqf_mixed, optimize_acqf_discrete_local_search, optimize_acqf_discrete\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "\n",
        "try:\n",
        "  import pointpats\n",
        "except:\n",
        "  !pip install pointpats\n",
        "  import pointpats\n",
        "#pointpats.random.poisson(get_all, size=1).tolist()\n",
        "\n",
        "MC_SAMPLES = 128\n",
        "\n",
        "# x = np.array([-80.5,39.5,0])\n",
        "# y = np.array([-75.5,42,1])\n",
        "# bounds = torch.stack([torch.Tensor(x), torch.Tensor(y)])\n",
        "#bounds = torch.tensor([[0.0] * 6, [1.0] * 6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TLOoM_SiuDT"
      },
      "outputs": [],
      "source": [
        "from math import e\n",
        "import numpy as np\n",
        "import random\n",
        "from shapely.ops import unary_union\n",
        "from shapely.geometry import Point\n",
        "#from shapely.geometry import MultiPoint\n",
        "from shapely.geometry.polygon import Polygon\n",
        "\n",
        "try:\n",
        "  import pointpats\n",
        "except:\n",
        "  !pip install pointpats\n",
        "  import pointpats\n",
        "\n",
        "from scipy.stats import qmc\n",
        "sampler = qmc.LatinHypercube(d=2)\n",
        "sample = sampler.random(n=10)\n",
        "\n",
        "\n",
        "l_bounds = [-79.21049943375089, 40.81555532075]\n",
        "u_bounds = [-78.70396386132802, 41.355336251154925]\n",
        "sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
        "\n",
        "x=map_df['geometry'].loc[0]\n",
        "bounds = map_df.geometry.apply(lambda x: x.bounds).tolist()\n",
        "point = Point(-80.07562,40.39311111)\n",
        "\n",
        "arr = df[0].to_numpy()\n",
        "arr_loc = np.zeros((len(map_df)))\n",
        "\n",
        "\n",
        "def get_index(arr,df):\n",
        "  for i in range(len(df.index)):\n",
        "    x=map_df['geometry'].loc[i]\n",
        "    point = Point(arr[0][0],arr[0][1])\n",
        "    if x.contains(point): #or x.touches(point):\n",
        "        return i\n",
        "\n",
        "def get_index_relative(arr,df):\n",
        "  loc=0\n",
        "  for i in range(len(df.index)):\n",
        "    x=map_df['geometry'].loc[i]\n",
        "    point = Point(arr[0][0],arr[0][1])\n",
        "    if x.contains(point): #or x.touches(point):\n",
        "        loc = i\n",
        "        break\n",
        "  index_ls = []\n",
        "  for i in range((len(df.index))):\n",
        "    if x.touches(df['geometry'].loc[i]):\n",
        "      index_ls.append(i)\n",
        "\n",
        "  pop_val = [pop[i] for i in index_ls]\n",
        "  return index_ls[np.argmin(pop_val)]\n",
        "\n",
        "def adjust_loc(arr, map_df):\n",
        "  e = 0.35\n",
        "  arr2 = [[arr[0][0],arr[0][1]]]\n",
        "  f1 = f2 = f3 = f4 = 0\n",
        "  if arr2[0][0] <= -80.48:\n",
        "    f1 = 1\n",
        "  if arr2[0][0] >= -75.28:\n",
        "    f2 = 1\n",
        "  if arr2[0][1] <= 39.72:\n",
        "    f3 = 1\n",
        "  if arr2[0][1] >= 48.9:\n",
        "    #was 42.25\n",
        "    f4 = 1\n",
        "\n",
        "  #else:\n",
        "    #arr[0][0] = arr[0][0] + e\n",
        "  if f1 == 1:\n",
        "    arr2[0][0] = arr2[0][0] + np.abs(-1*arr2[0][0]-80.48)  + e\n",
        "  if f2 == 1:\n",
        "    arr2[0][0] = arr2[0][0] - np.abs(-1*arr2[0][0]-75.28)  - e\n",
        "  if f3 == 1:\n",
        "    arr2[0][1] = arr2[0][1] - np.abs(arr2[0][1]-39.72)  + e\n",
        "  if f4 == 1:\n",
        "    arr2[0][1] = arr2[0][1] - np.abs(arr2[0][1]-42.2)  - e\n",
        "\n",
        "  if torch.is_tensor(arr2[0][0]) or torch.is_tensor(arr2[0][1]):\n",
        "    arr2[0][0] = arr2[0][0].detach().tolist()\n",
        "    arr2[0][1] = arr2[0][1].detach().tolist()\n",
        "  arr_ls = [arr2[0][0], arr2[0][1]]\n",
        "  for i in range(2,len(arr[0])):\n",
        "    #print(\"here:\",arr,i)\n",
        "    if torch.is_tensor(arr[0][i]):\n",
        "      arr_ls.append(arr[0][i].detach().tolist())\n",
        "    else:\n",
        "      arr_ls.append(arr[0][i])\n",
        "\n",
        "  return [arr_ls]\n",
        "\n",
        "def get_random_point(arr, map_df):\n",
        "  get_all = unary_union( map_df['geometry'].tolist() )\n",
        "  arr_ls = pointpats.random.poisson(get_all, size=1).tolist()\n",
        "  for i in range(2,len(arr[0])):\n",
        "    #print(\"here:\",arr,i)\n",
        "    if torch.is_tensor(arr[0][i]):\n",
        "      arr_ls.append(arr[0][i].detach().tolist())\n",
        "    else:\n",
        "      arr_ls.append(arr[0][i])\n",
        "\n",
        "  return [arr_ls]\n",
        "\n",
        "def random_points_in_polygon(number, polygon):\n",
        "    points = []\n",
        "    ls = []\n",
        "    min_x, min_y, max_x, max_y = polygon.bounds\n",
        "    i= 0\n",
        "    while i < number:\n",
        "        x1 = random.uniform(min_x, max_x)\n",
        "        y1 = random.uniform(min_y, max_y)\n",
        "        point = Point(x1, y1)\n",
        "        ls_new = [x1,y1]\n",
        "        if polygon.contains(point):\n",
        "            points.append(point)\n",
        "            ls.append(ls_new)\n",
        "            i += 1\n",
        "    return ls\n",
        "\n",
        "import geopandas as gpd\n",
        "\n",
        "def get_neighboring_counties(state_name, map_df, county_name):\n",
        "    \"\"\"\n",
        "    Function to return the neighboring counties of a given county in the same state.\n",
        "\n",
        "    Args:\n",
        "        state_name (str): Name of the state (e.g., \"North Dakota\").\n",
        "        county_name (str): Name of the county (e.g., \"Steele\").\n",
        "\n",
        "    Returns:\n",
        "        List of neighboring county names.\n",
        "    \"\"\"\n",
        "    # Load the U.S. counties shapefile (change the file path to your shapefile location)\n",
        "    counties = gpd.read_file(\"https://www2.census.gov/geo/tiger/GENZ2022/shp/cb_2022_us_county_5m.zip\")\n",
        "\n",
        "    # Filter by the specific state\n",
        "    state_counties = counties[counties['STATE_NAME'].str.lower() == state_name.lower()]\n",
        "\n",
        "    # Find the target county\n",
        "    target_county = state_counties[state_counties['NAME'].str.lower() == county_name.lower()]\n",
        "\n",
        "    if target_county.empty:\n",
        "        print(f\"County '{county_name}' not found in '{state_name}'.\")\n",
        "        return []\n",
        "\n",
        "    # Find neighboring counties using spatial intersection (buffer to account for boundary precision)\n",
        "    neighbors = state_counties[state_counties.geometry.touches(target_county.geometry.iloc[0])]\n",
        "    neighbors = neighbors['NAME'].tolist()\n",
        "    print(neighbors)\n",
        "    # Return the names of neighboring counties\n",
        "    names = map_df['NAME'].tolist()\n",
        "    county_indexes = [names.index(i) for i in neighbors]\n",
        "    sample_size = [np.shape(train_gp_LR1[i])[0] for i in (county_indexes)]\n",
        "    #neighbor_names = neighbors['NAME'].tolist()\n",
        "    return np.argmin(sample_size)\n",
        "\n",
        "\n",
        "def random_points_in_polygon_2(number, polygon):\n",
        "  sampler = qmc.LatinHypercube(d=2)\n",
        "  sample = sampler.random(n=10)\n",
        "  points = []\n",
        "  ls = []\n",
        "  min_x, min_y, max_x, max_y = polygon.bounds\n",
        "  i= 0\n",
        "  l_bounds = [min_x, min_y]\n",
        "  u_bounds = [max_x, max_y]\n",
        "\n",
        "  while i < number:\n",
        "        sample = sampler.random(n=number)\n",
        "        ls_pnt = qmc.scale(sample, l_bounds, u_bounds)\n",
        "        x1 = ls_pnt[0][0]\n",
        "        y1 = ls_pnt[0][1]\n",
        "        point = Point(x1, y1)\n",
        "        ls_new = [x1,y1]\n",
        "        if polygon.contains(point):\n",
        "            points.append(point)\n",
        "            ls.append(ls_new)\n",
        "            i += 1\n",
        "  return ls\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "def get_random(arr):\n",
        "  sampler = qmc.LatinHypercube(d=2)\n",
        "  sample = sampler.random(n=10)\n",
        "  points = []\n",
        "  ls = []\n",
        "  min_x, min_y, max_x, max_y = arr\n",
        "  i= 0\n",
        "  l_bounds = [min_x, min_y]\n",
        "  u_bounds = [max_x, max_y]\n",
        "\n",
        "  while 1:\n",
        "    sample = sampler.random(n=1)\n",
        "    ls_pnt = qmc.scale(sample, l_bounds, u_bounds)\n",
        "    x1 = ls_pnt[0][0]\n",
        "    y1 = ls_pnt[0][1]\n",
        "    ls_new = [x1,y1]\n",
        "    point = Point(x1, y1)\n",
        "    ls = map_df.contains(point).to_list()\n",
        "    val =reduce(lambda x, y: x+y, ls)\n",
        "    if val:\n",
        "        break\n",
        "  return ls_new\n",
        "\n",
        "random_points_in_polygon_2(1,map_df['geometry'].iloc[32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLYpRmF3BRUE"
      },
      "outputs": [],
      "source": [
        "if np.shape(ls)[1] > 2:\n",
        "  newls = np.zeros((len(map_df),np.shape(ls)[1]))\n",
        "else:\n",
        "  newls = np.zeros((len(map_df),2))\n",
        "for i in range(len(map_df['geometry'])):\n",
        "  newls[i,0]=map_df['geometry'].loc[i].centroid.x\n",
        "  newls[i,1]=map_df['geometry'].loc[i].centroid.y\n",
        "  if np.shape(ls)[1] > 2:\n",
        "    for j in range(2,np.shape(ls)[1]):\n",
        "      #newls[i,j]=feature[j-2][i]\n",
        "      #newls[i,2]=income[i]\n",
        "      newls[i,2]=pop_den[i]\n",
        "      #newls[i,4]=black_p[i]\n",
        "      #newls[i,5]=native_p[i]\n",
        "      #newls[i,4]=black_percent[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MJ0kaQ9wQQ7"
      },
      "outputs": [],
      "source": [
        "ls_nam = map_df['NAME'].to_list()\n",
        "#print(ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g420oYVCcD0h"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "import gpytorch\n",
        "from botorch.posteriors.gpytorch import scalarize_posterior_gpytorch\n",
        "from botorch.acquisition.objective import ScalarizedPosteriorTransform\n",
        "from botorch.acquisition.objective import PosteriorTransform\n",
        "from botorch.acquisition.analytic import ExpectedImprovement\n",
        "from botorch.acquisition.max_value_entropy_search import qMaxValueEntropy\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "\n",
        "from botorch.models import HigherOrderGP, SingleTaskGP\n",
        "\n",
        "from botorch.optim import optimize_acqf_discrete_local_search\n",
        "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
        "from botorch.models.model import Model\n",
        "from botorch.sampling.base import MCSampler\n",
        "from botorch.sampling.normal import SobolQMCNormalSampler\n",
        "from botorch.utils import t_batch_mode_transform\n",
        "from botorch.acquisition.objective import PosteriorTransform\n",
        "from botorch.models import MultiTaskGP\n",
        "from gpytorch.distributions import MultivariateNormal\n",
        "\n",
        "#from botorch.models import FixedNoiseGP, MultiTaskGP\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "from botorch.acquisition import AnalyticAcquisitionFunction\n",
        "\n",
        "\n",
        "class MultiOutputGPModel(MultiTaskGP):\n",
        "    def __init__(self, train_X, train_Y):\n",
        "        #super().__init__(train_X, train_Y, output_tasks=train_Y.shape[1], task_feature=1 )\n",
        "        super().__init__(train_X, train_Y, num_outputs=train_Y.shape[1])\n",
        "\n",
        "        # Ensure we're using an appropriate likelihood for multi-output regression\n",
        "        self.likelihood = self._get_likelihood()\n",
        "\n",
        "    def _get_likelihood(self):\n",
        "        return super().likelihood\n",
        "\n",
        "\n",
        "class Entropy(AnalyticAcquisitionFunction):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Model,\n",
        "        posterior_transform: Optional[PosteriorTransform] = None,\n",
        "        maximize: bool = True,\n",
        "    ) -> None:\n",
        "        r\"\"\"Single-outcome Posterior Mean.\n",
        "\n",
        "        Args:\n",
        "            model: A fitted single-outcome GP model (must be in batch mode if\n",
        "                candidate sets X will be)\n",
        "            posterior_transform: A PosteriorTransform. If using a multi-output model,\n",
        "                a PosteriorTransform that transforms the multi-output posterior into a\n",
        "                single-output posterior is required.\n",
        "            maximize: If True, consider the problem a maximization problem. Note\n",
        "                that if `maximize=False`, the posterior standard deviation is negated.\n",
        "                As a consequence,\n",
        "                `optimize_acqf(PosteriorStandardDeviation(gp, maximize=False))`\n",
        "                actually returns -1 * minimum of the posterior standard deviation.\n",
        "        \"\"\"\n",
        "        super().__init__(model=model, posterior_transform=posterior_transform)\n",
        "        self.maximize = maximize\n",
        "\n",
        "    @t_batch_mode_transform(expected_q=1)\n",
        "    def forward(self, X: Tensor) -> Tensor:\n",
        "        r\"\"\"Evaluate the posterior standard deviation on the candidate set X.\n",
        "\n",
        "        Args:\n",
        "            X: A `(b1 x ... bk) x 1 x d`-dim batched tensor of `d`-dim design points.\n",
        "\n",
        "        Returns:\n",
        "            A `(b1 x ... bk)`-dim tensor of Posterior Mean values at the\n",
        "            given design points `X`.\n",
        "        \"\"\"\n",
        "        _, std = self._mean_and_sigma(X)\n",
        "        #1/2 log (2\\pi\\sigma^2)+0.5\n",
        "        #entropy = -1*0.5*torch.log(2*math.pi*std**2) + 0.001\n",
        "        #entropy = 0.5*torch.log(std*math.sqrt(2*math.pi*math.exp(1)) ) + 0.0001\n",
        "        entropy = 0.5*torch.log(2*math.pi*std**2) + 0.5\n",
        "        return entropy if self.maximize else -1*entropy\n",
        "\n",
        "# ----------------Signal to noise ratio AF ------------------------------\n",
        "class SignalToNoiseRatio(AnalyticAcquisitionFunction):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Model,\n",
        "        posterior_transform: Optional[PosteriorTransform] = None,\n",
        "        jitter=1e-6, max_tries=10,\n",
        "        maximize: bool = True,\n",
        "    ) -> None:\n",
        "        r\"\"\"Single-outcome Posterior Mean.\n",
        "\n",
        "        Args:\n",
        "            model: A fitted single-outcome GP model (must be in batch mode if\n",
        "                candidate sets X will be)\n",
        "            posterior_transform: A PosteriorTransform. If using a multi-output model,\n",
        "                a PosteriorTransform that transforms the multi-output posterior into a\n",
        "                single-output posterior is required.\n",
        "            maximize: If True, consider the problem a maximization problem. Note\n",
        "                that if `maximize=False`, the posterior standard deviation is negated.\n",
        "                As a consequence,\n",
        "                `optimize_acqf(PosteriorStandardDeviation(gp, maximize=False))`\n",
        "                actually returns -1 * minimum of the posterior standard deviation.\n",
        "        \"\"\"\n",
        "        super().__init__(model=model, posterior_transform=posterior_transform)\n",
        "        self.jitter   = jitter\n",
        "        self.max_tries = max_tries\n",
        "        self.maximize = maximize\n",
        "\n",
        "    @t_batch_mode_transform(expected_q=1)\n",
        "    def forward(self, X: Tensor) -> Tensor:\n",
        "        r\"\"\"Evaluate the posterior standard deviation on the candidate set X.\n",
        "\n",
        "        Args:\n",
        "            X: A `(b1 x ... bk) x 1 x d`-dim batched tensor of `d`-dim design points.\n",
        "\n",
        "        Returns:\n",
        "            A `(b1 x ... bk)`-dim tensor of Posterior Mean values at the\n",
        "            given design points `X`.\n",
        "        \"\"\"\n",
        "        mu, std = self._mean_and_sigma(X)\n",
        "        #print(mu,std)\n",
        "        ratio = mu/std + 0.001\n",
        "        ratio = (2*1.96*std)/mu + 0.01\n",
        "        ratio = abs(std/mu) + 0.01\n",
        "\n",
        "        return ratio if self.maximize else -1*ratio\n",
        "        \"\"\"\n",
        "        #self.to(device=X.device)  # ensures buffers / parameters are on the same device\n",
        "        posterior = self.model.posterior(\n",
        "            X=X, posterior_transform=self.posterior_transform\n",
        "        )\n",
        "        mu = posterior.mean\n",
        "        sigma = posterior.variance.sqrt().clamp_min(1e-9)  # Avoid division by zero\n",
        "        ratio = mu / sigma\n",
        "        return ratio if self.maximize else -ratio\n",
        "        \"\"\"\n",
        "\n",
        "    def _make_pd(self, cov: torch.Tensor) -> torch.Tensor:\n",
        "    # symmetrize\n",
        "      cov = 0.5 * (cov + cov.transpose(-1, -2))\n",
        "      jit = self.jitter\n",
        "      eye   = torch.eye(cov.size(-1), device=cov.device)\n",
        "      for _ in range(self.max_tries):\n",
        "          try:\n",
        "              return torch.linalg.cholesky(cov + jit * eye)\n",
        "          except RuntimeError:\n",
        "              jit *= 10\n",
        "      raise RuntimeError(f\"Could not PD‐fix cov with jitter up to {jit:e}\")\n",
        "\n",
        "    def posterior(self, X: torch.Tensor, **kwargs) -> MultivariateNormal:\n",
        "      # get the \"raw\" GP posterior\n",
        "      post = super().posterior(X, **kwargs)\n",
        "      L    = self._make_pd(post.covariance_matrix)\n",
        "      # build a new (safe) MultivariateNormal\n",
        "      return MultivariateNormal(post.mean, scale_tril=L)\n",
        "# ----------------Signal to noise ratio+Entropy AF ------------------------------\n",
        "class EntropySNR(AnalyticAcquisitionFunction):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Model,\n",
        "        posterior_transform: Optional[PosteriorTransform] = None,\n",
        "        maximize: bool = True,\n",
        "    ) -> None:\n",
        "        r\"\"\"Single-outcome Posterior Mean.\n",
        "\n",
        "        Args:\n",
        "            model: A fitted single-outcome GP model (must be in batch mode if\n",
        "                candidate sets X will be)\n",
        "            posterior_transform: A PosteriorTransform. If using a multi-output model,\n",
        "                a PosteriorTransform that transforms the multi-output posterior into a\n",
        "                single-output posterior is required.\n",
        "            maximize: If True, consider the problem a maximization problem. Note\n",
        "                that if `maximize=False`, the posterior standard deviation is negated.\n",
        "                As a consequence,\n",
        "                `optimize_acqf(PosteriorStandardDeviation(gp, maximize=False))`\n",
        "                actually returns -1 * minimum of the posterior standard deviation.\n",
        "        \"\"\"\n",
        "        super().__init__(model=model, posterior_transform=posterior_transform)\n",
        "        self.maximize = maximize\n",
        "\n",
        "    @t_batch_mode_transform(expected_q=1)\n",
        "    def forward(self, X: Tensor) -> Tensor:\n",
        "        r\"\"\"Evaluate the posterior standard deviation on the candidate set X.\n",
        "\n",
        "        Args:\n",
        "            X: A `(b1 x ... bk) x 1 x d`-dim batched tensor of `d`-dim design points.\n",
        "\n",
        "        Returns:\n",
        "            A `(b1 x ... bk)`-dim tensor of Posterior Mean values at the\n",
        "            given design points `X`.\n",
        "        \"\"\"\n",
        "        mu, std = self._mean_and_sigma(X)\n",
        "        #ratio = 0.5*torch.log(2*math.pi*std**2) - mu/std + 0.5\n",
        "        ratio = 0.5*torch.log(std*math.sqrt(2*math.pi*math.exp(1)) ) + 0.5*std/mu\n",
        "        return ratio if self.maximize else -ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKXF5Mu3qtZO"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets, linear_model\n",
        "from sklearn import datasets, linear_model\n",
        "\n",
        "start = 0\n",
        "# was 2020\n",
        "num = 1000\n",
        "loc = 1\n",
        "\n",
        "X = []\n",
        "for j in range(5):\n",
        "  for k in range(5):\n",
        "    for i in range(start,num):\n",
        "      X.append([1,j,k])\n",
        "\n",
        "train_Y_LR_total = [[] for i in range(len(map_df['NAME'].tolist()))]\n",
        "reg_array = [[] for i in range(len(map_df['NAME'].tolist()))]\n",
        "\n",
        "for loc in range(len(map_df['NAME'].tolist())):\n",
        "  train_Y_LR_total[loc] = np.array([df[0][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[1][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[2][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[3][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[4][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[5][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[6][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[7][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[8][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[9][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[10][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[11][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[12][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[13][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[14][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[15][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[16][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[17][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[18][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[19][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[20][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[21][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[22][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[23][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "                                    + df[24][map_df['NAME'].loc[loc]].tolist()[start:num]\n",
        "\n",
        "                                    ]).T\n",
        "  reg_array[loc] = linear_model.LinearRegression()\n",
        "  #print(np.shape(X), np.shape(train_Y_LR) )\n",
        "  # Train the model using the training sets\n",
        "  reg_array[loc].fit(X, train_Y_LR_total[loc])\n",
        "\n",
        "  #y_pred = regr.predict(X)\n",
        "  print(reg_array[loc].intercept_,reg_array[loc].coef_)\n",
        "  #print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axPux9B0nS4p"
      },
      "outputs": [],
      "source": [
        "\n",
        "# start_ls = 1000\n",
        "# end = 2000\n",
        "start_ls = 500\n",
        "end = 1000\n",
        "df_avg = [[] for i in range(25)]\n",
        "\n",
        "for j in range(5):\n",
        "  for k in range(5):\n",
        "    for i in range(len(map_df)):\n",
        "      #print(j*5+k,i, np.shape(df_avg[0]))\n",
        "      df_avg[j*5+k].append(np.array(df[j*5+k][map_df['NAME'].loc[i]].tolist()[start_ls:end] ).mean() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DcgNdUrCc4l"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_avg_all = []\n",
        "for i in range(len(map_df)):\n",
        "  temp = []\n",
        "  for j in range(25):\n",
        "    temp.append(df_avg[j][i])\n",
        "\n",
        "  df_avg_all.append(np.array(temp).mean())\n",
        "  #df_avg_all.append(np.array(df_avg[i]).mean())\n",
        "\n",
        "lr1_true = np.array([reg_array[i].intercept_[0] for i in range(len(map_df))])\n",
        "lr2_true = np.array([reg_array[i].coef_[0][1] for i in range(len(map_df))])\n",
        "lr3_true = np.array([reg_array[i].coef_[0][2] for i in range(len(map_df))])\n",
        "#lr4 = np.array([reg_array[i].coef_[0][3] for i in range(len(map_df))])\n",
        "#lr5 = np.array([reg_array[i].coef_[0][4] for i in range(len(map_df))])\n",
        "#lr6 = np.array([reg_array[i].coef_[0][5] for i in range(len(map_df))])\n",
        "\n",
        "error = []\n",
        "total_value = len(map_df)\n",
        "pop_100 = [100000 for i in range(total_value)]\n",
        "error = []\n",
        "val = []; val2 = []\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    #error.append( (df_avg[i*5+j]-(lr1+lr2*i+lr3*j) )**2 )\n",
        "    val.append(sum((df_avg[i*5+j] - (lr1_true+lr2_true*i+lr3_true*j) )**2)/total_value)\n",
        "    val2.append(sum(( (df_avg[i*5+j]/np.array(pop))*np.array(pop_100) - (np.array(lr1_true+lr2_true*i+lr3_true*j)/np.array(pop))*np.array(pop_100) )**2)/total_value)\n",
        "\n",
        "\n",
        "sum(val)/25, sum(val2)/25\n",
        "#get mean over all counties and all TCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5gfvg1KEBFG"
      },
      "outputs": [],
      "source": [
        "bound0 = [-104,45.9,30,0,5]\n",
        "bound1 = [-96.3,48.9,100,110,300]\n",
        "def find_widest_ci_conditions(samples, num_levels1, num_levels2):\n",
        "    \"\"\"\n",
        "    Given posterior samples of shape (S, n_counties, 3) where each draw is\n",
        "    [intercept, beta_level1, beta_level2], and the number of levels for each\n",
        "    factor, compute for each county which treatment‐condition index has the\n",
        "    widest 95% credible interval of the predicted outcome.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    samples : array‐like, shape (S, n_counties, 3)\n",
        "        Posterior draws for intercept and two coefficients.\n",
        "    num_levels1 : int\n",
        "        Number of discrete levels for the first factor (e.g., naloxone).\n",
        "    num_levels2 : int\n",
        "        Number of discrete levels for the second factor (e.g., buprenorphine).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    widest_idx : ndarray, shape (n_counties,)\n",
        "        For each county, the integer index (0 to num_levels1*num_levels2−1)\n",
        "        of the treatment condition with the largest 95% CI width.\n",
        "    \"\"\"\n",
        "    arr = np.array(samples)  # shape (S, n_counties, 3)\n",
        "    S, n_counties, _ = arr.shape\n",
        "\n",
        "    # Build list of all treatment conditions (level1, level2)\n",
        "    levels = [\n",
        "        (l1, l2)\n",
        "        for l1 in range(1, num_levels1 + 1)\n",
        "        for l2 in range(1, num_levels2 + 1)\n",
        "    ]\n",
        "    C = len(levels)\n",
        "\n",
        "    # Compute predicted outcomes for each draw, county, and condition\n",
        "    y = np.zeros((S, n_counties, C))\n",
        "    for c, (l1, l2) in enumerate(levels):\n",
        "        y[:, :, c] = arr[:, :, 0] + arr[:, :, 1] * l1 + arr[:, :, 2] * l2\n",
        "\n",
        "    # 95% credible intervals\n",
        "    lower = np.percentile(y, 2.5, axis=0)   # shape (n_counties, C)\n",
        "    upper = np.percentile(y, 97.5, axis=0)  # shape (n_counties, C)\n",
        "\n",
        "    # Widths and argmax\n",
        "    widths = upper - lower                  # shape (n_counties, C)\n",
        "    widest_idx = np.argmax(widths, axis=1)  # shape (n_counties,)\n",
        "\n",
        "    return widest_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwIp1uwlGJck"
      },
      "source": [
        "\n",
        "\n",
        "# Running GPR-RF\n",
        "\n",
        "In this section, we implement the two-step metamodel: a multi-output Gaussian Process Regression (MO–GPR) with response function (RF). The MO-GPR serves as a surrogate for the expensive simulation model, capturing how epidemic outcomes respond to treatment allocations across counties.\n",
        "\n",
        "Let each county be indexed by\n",
        "$c \\in \\mathcal{C}$\n",
        "and each treatment condition by\n",
        "$(n,b) \\in \\mathcal{T}$,\n",
        "where $n$ and $b$ represent naloxone and buprenorphine intervention levels, respectively.\n",
        "\n",
        "We denote the simulation output for county $c$ under treatment $(n,b)$ at design iteration $t$ as\n",
        "$\n",
        "y.\n",
        "$\n",
        "\n",
        "The dataset at iteration $t$ is:\n",
        "$\n",
        "\\mathcal{D}_t = \\mathcal{D}_{t-1} \\cup \\{ (\\mathbf{x}_c, (c, n, b), y) \\},\n",
        "$\n",
        "where $\\mathbf{x}_c$ includes spatial features (county centroid) and contextual features (population, overdose trends, dispensing rates).\n",
        "\n",
        "The MO–GPR posterior for a new input $x_c^*$ is:\n",
        "\\[\n",
        "$\\boldsymbol{\\mu}(\\mathbf{x}_c^*)$ =\n",
        "\\begin{bmatrix}\n",
        "\\mu_0(\\mathbf{x}_c^*),\n",
        "\\mu_n(\\mathbf{x}_c^*),\n",
        "\\mu_b(\\mathbf{x}_c^*)\n",
        "\\end{bmatrix},\n",
        "$\\boldsymbol{K}(\\mathbf{x}_c^*)$\n",
        "\\]\n",
        "\n",
        "Here, $\\mu_0, \\mu_n, \\mu_b$ are regression coefficients approximating baseline mortality and treatment effects. Their uncertainty, encoded in $K$, is estimated from replicate simulation runs.\n",
        "\n",
        "This section of code therefore:\n",
        "1. Defines the kernel.\n",
        "2. Fits the GPR to current simulation data.\n",
        "3. Samples posterior mean and variance for each county-treatment combination.\n",
        "4. Updates the metamodel to guide the sequential design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmCpUSoOwwfD"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD, Adam, LBFGS, RMSprop, Adagrad\n",
        "from botorch.utils.transforms import normalize, unnormalize\n",
        "from botorch.models.transforms.input import Normalize\n",
        "from botorch.optim.initializers import initialize_q_batch_nonneg\n",
        "#from botorch.optim.fit import fit_gpytorch_torch\n",
        "\n",
        "import copy\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "torch.set_warn_always(True)\n",
        "\n",
        "step = 10\n",
        "batch_size = 5\n",
        "\n",
        "batch_size2 = batch_size+step\n",
        "loc = 1\n",
        "normalize_flag = False\n",
        "################################################################################################\n",
        "X_levels = []\n",
        "\n",
        "for j in range(5):\n",
        "  for k in range(5):\n",
        "    for i in range(batch_size):\n",
        "      X_levels.append([1,j,k])\n",
        "\n",
        "train_gp_LR = [[] for i in range(len(map_df['NAME'].tolist()))]\n",
        "reg_array_gp = [[] for i in range(len(map_df['NAME'].tolist()))]\n",
        "\n",
        "for loc in range(len(map_df['NAME'].tolist())):\n",
        "  train_gp_LR[loc] = np.array([df[0][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                               + df[1][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                               + df[2][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                               + df[3][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                               + df[4][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[5][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[6][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[7][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[8][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[9][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[10][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[11][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[12][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[13][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[14][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[15][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[16][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[17][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[18][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[19][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[20][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[21][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[22][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[23][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                              + df[24][map_df['NAME'].loc[loc]].tolist()[:batch_size]\n",
        "                               ]).T\n",
        "\n",
        "  reg_array_gp[loc] = linear_model.LinearRegression()\n",
        "  # Train the model using the training sets\n",
        "  reg_array_gp[loc].fit(X_levels, train_gp_LR[loc])\n",
        "\n",
        "\n",
        "train_reg1 = torch.Tensor([round(reg_array_gp[i].intercept_[0],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "train_reg2 = torch.Tensor([round(reg_array_gp[i].coef_[0][1],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "train_reg3 = torch.Tensor([round(reg_array_gp[i].coef_[0][2],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "\n",
        "#train_reg4 = torch.Tensor([round(reg_array_gp[i].coef_[0][3],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "#train_reg5 = torch.Tensor([round(reg_array_gp[i].coef_[0][4],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "\n",
        "test_reg1 = torch.Tensor([round(reg_array[i].intercept_[0],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "test_reg2 = torch.Tensor([round(reg_array[i].coef_[0][1],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "test_reg3 = torch.Tensor([round(reg_array[i].coef_[0][2],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "#test_reg4 = torch.Tensor([round(reg_array[i].coef_[0][3],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "#test_reg5 = torch.Tensor([round(reg_array[i].coef_[0][4],3) for i in range(len(map_df['NAME'].tolist()))])\n",
        "\n",
        "par1 = [reg_array[i].intercept_[0] for i in range(len(map_df['NAME'].tolist()))]\n",
        "par2 = [reg_array[i].coef_[0][1] for i in range(len(map_df['NAME'].tolist()))]\n",
        "par3 = [reg_array[i].coef_[0][2] for i in range(len(map_df['NAME'].tolist()))]\n",
        "#par4 = [reg_array[i].coef_[0][3] for i in range(len(map_df['NAME'].tolist()))]\n",
        "#par5 = [reg_array[i].coef_[0][4] for i in range(len(map_df['NAME'].tolist()))]\n",
        "\n",
        "################################################################################################\n",
        "true_intercepts = 2.0 + np.random.rand() * 0.002 * train_X.sum(dim=1)\n",
        "true_slopes = 3.0 + np.random.rand() * 0.001 * train_X.sum(dim=1)\n",
        "count1 = [0 for i in range(len(map_df))]\n",
        "count2 = [0 for i in range(len(map_df))]\n",
        "count3 = [0 for i in range(len(map_df))]\n",
        "count4 = [0 for i in range(len(map_df))]\n",
        "\n",
        "if normalize_flag:\n",
        "  bounds1 = [train_X.min(), train_X.max()]\n",
        "  bounds2 = [train_reg1.min(), train_reg1.max()]\n",
        "  bounds3 = [train_reg2.min(), train_reg2.max()]\n",
        "  train_X = normalize(train_X, bounds1)\n",
        "  #train_Y = normalize(train_Y, bounds2)\n",
        "  train_reg1 = normalize(train_reg1, bounds2)\n",
        "  train_reg2 = normalize(train_reg2, bounds3)\n",
        "\n",
        "num_par = 3\n",
        "ls_y = [[round(train_reg1[i].tolist(),3), round(train_reg2[i].tolist(),3), round(train_reg3[i].tolist(),3),\n",
        "         #round(train_reg4[i].tolist(),3), round(train_reg5[i].tolist(),3)\n",
        "         ]\n",
        "        for i in range(len(map_df))]\n",
        "ls_y_test = [[round(test_reg1[i].tolist(),3), round(test_reg2[i].tolist(),3), round(test_reg3[i].tolist(),3),\n",
        "         #round(test_reg4[i].tolist(),3), round(test_reg5[i].tolist(),3)\n",
        "         ]\n",
        "        for i in range(len(map_df))]\n",
        "train_Y = torch.tensor(ls_y, dtype=torch.float64)\n",
        "test_Y = torch.tensor(ls_y_test, dtype=torch.float64)\n",
        "\n",
        "train_X1 = train_X\n",
        "test_X = copy.deepcopy(train_X)\n",
        "train_X2 = train_X\n",
        "train_X3 = train_X\n",
        "train_X4 = train_X\n",
        "train_X5 = train_X\n",
        "train_Y1 = train_Y\n",
        "train_Y2 = train_Y\n",
        "train_Y3 = train_Y\n",
        "\n",
        "compare = 2\n",
        "covar_module_comp = [0 for i in range(compare)]\n",
        "comp_flag = False\n",
        "\n",
        "train_reg5 = train_reg1\n",
        "\n",
        "train_gp_LR1 = train_gp_LR\n",
        "train_gp_LR2 = train_gp_LR\n",
        "train_gp_LR3 = train_gp_LR\n",
        "train_gp_LR4 = train_gp_LR\n",
        "train_gp_LR5 = train_gp_LR\n",
        "reg_array_gp1 = reg_array_gp\n",
        "reg_array_gp2 = reg_array_gp\n",
        "reg_array_gp3 = reg_array_gp\n",
        "reg_array_gp4 = reg_array_gp\n",
        "reg_array_gp5 = reg_array_gp\n",
        "\n",
        "mse_all = []\n",
        "mse_all2 = []\n",
        "mse_all3 = []\n",
        "mse_all_comp = [[] for i in range(compare)]\n",
        "mse_all_comp2 = [[] for i in range(compare)]\n",
        "gp_value = []\n",
        "gp_value2 = []\n",
        "gp_rate = []\n",
        "gp_rate2 = []\n",
        "gp_rate3 = []\n",
        "gp_rate4 = []\n",
        "adams_mse_rate = []\n",
        "adams_mse_count = []\n",
        "bge_all = []\n",
        "bge_all2 = []\n",
        "sample_complexity = []\n",
        "sample_complexity2 = []\n",
        "samples = [0]*len(map_df)\n",
        "\n",
        "flag = False\n",
        "\n",
        "entropy_flag = False\n",
        "\n",
        "num_samples = [list(i.flatten()) for i in train_gp_LR1]\n",
        "sum_num_samples = sum(xyz, [])\n",
        "print(\"shape of train set:\", np.shape(sum_num_samples))\n",
        "#train_val1 = train_reg.detach().numpy().flatten()\n",
        "#train_val2 = train_Y2.detach().numpy().flatten()\n",
        "\n",
        "N_BATCH = 150 #int(20000/step) #was 2000\n",
        "\n",
        "covar_module = (ScaleKernel(RBFKernel(active_dims=[0,1]))+(RBFKernel(active_dims=[2])+RBFKernel(active_dims=[3]))*PeriodicKernel())\n",
        "\n",
        "covar_module = RBFKernel(active_dims=[0,1])+RBFKernel(active_dims=[2])#*PeriodicKernel()\n",
        "\n",
        "gp = SingleTaskGP(train_X1, train_Y.double(),\n",
        "                  #outcome_transform=Standardize(m=num_par),\n",
        "                  #input_transform=Normalize(d=train_X.shape[-1]),\n",
        "                   #outcome_transform=None,\n",
        "                   covar_module=covar_module\n",
        "                  )\n",
        "\n",
        "gp_mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "\n",
        "fit_gpytorch_mll(mll=gp_mll)\n",
        "\n",
        "val = round(1.0/num_par,3)\n",
        "ls_val = [val for i in range(num_par)]\n",
        "#ls_val = [0.8,0.1,0.1]\n",
        "weights = torch.tensor(ls_val, dtype=torch.double)\n",
        "posterior_transform = ScalarizedPosteriorTransform(weights)\n",
        "\n",
        "SNR = SignalToNoiseRatio(gp.double(), posterior_transform=posterior_transform, jitter=1e-2, max_tries=10)\n",
        "\n",
        "iteration = 0\n",
        "round_robin = 5\n",
        "stop_sample = [[batch_size]*5*5 for i in range(len(map_df))]\n",
        "\n",
        "while iteration < N_BATCH:\n",
        "  iteration += 1\n",
        "  #was 0.07\n",
        "  lr_val = max(0.07, 0.3 - iteration*0.0000001)\n",
        "  with gpytorch.settings.cholesky_jitter(1e-4):\n",
        "\n",
        "    training_iter = 1\n",
        "\n",
        "    fit_gpytorch_mll(mll=gp_mll)\n",
        "\n",
        "    SNR = SignalToNoiseRatio(gp.double(), maximize=True, posterior_transform=posterior_transform,\n",
        "                               jitter=1e-2, max_tries=10)\n",
        "\n",
        "    C = choice.T\n",
        "    C = torch.as_tensor(C, dtype=torch.float64, device=gp.train_inputs[0].device)\n",
        "\n",
        "    for _ in range(20):                      # retry up to 5 times\n",
        "      try:\n",
        "          new_pnt, new_value = optimize_acqf_discrete(\n",
        "          acq_function=SNR.double(),\n",
        "          #bounds= bounds3,\n",
        "          q=1,\n",
        "          choices = C,\n",
        "          )\n",
        "          break\n",
        "      except RuntimeError as e:\n",
        "          if \"cholesky\" in str(e).lower():\n",
        "              #gp.likelihood.noise_covar.noise.data *= 0.5 #1.1   # bump noise, retry\n",
        "              with torch.no_grad():\n",
        "                # Increase noise and ensure it's double precision\n",
        "                gp.likelihood.noise_covar.noise.data = (\n",
        "                    gp.likelihood.noise_covar.noise.data.double().clamp(min=1e-6) #* 1.5\n",
        "                )\n",
        "                #print(\"noise:\",gp.likelihood.noise_covar.noise.data.double())\n",
        "                # Force all model parameters to double\n",
        "                for param in gp.parameters():\n",
        "                    if param.dtype != torch.float64:\n",
        "                        param.data = param.data.double()\n",
        "          else:\n",
        "              raise\n",
        "\n",
        "  idx = get_index(new_pnt,map_df)\n",
        "  if idx is None:\n",
        "    print(\"Here in idx:\", new_pnt)\n",
        "    new_pnt22 = adjust_loc(new_pnt, map_df)\n",
        "    print(\"Here in idx:\", new_pnt22)\n",
        "    new_pnt = new_pnt22\n",
        "    #print(pointsame1, counter)\n",
        "    idx = get_index(new_pnt,map_df)\n",
        "\n",
        "  if flag or max(samples)>48000:\n",
        "    print(\"Choosing one of the neighbors as the idx has exhausted all samples\")\n",
        "    state = \"North Dakota\"\n",
        "    county = map_df['NAME'].loc[idx]\n",
        "    idx = get_neighboring_counties(state, map_df, county)\n",
        "    old_pnt = new_pnt\n",
        "    new_pnt = torch.tensor([newls[idx]])\n",
        "    print(idx, map_df['NAME'].loc[idx], old_pnt, new_pnt)\n",
        "    flag = False\n",
        "\n",
        "  loc = idx\n",
        "  samples = [np.shape(train_gp_LR1[i])[0] for i in range(len(map_df))]\n",
        "  #if samples[0] <= batch_size*5:\n",
        "  #  idx = 0\n",
        "  #  loc = idx\n",
        "  loc = idx\n",
        "  if iteration <= 1:\n",
        "    ci_ind = 0\n",
        "  else:\n",
        "    ci_ind = widest_indices[loc]\n",
        "  train_gp_LR1[idx] = np.append(train_gp_LR1[idx], df[ci_ind][map_df['NAME'].loc[loc]].tolist()[stop_sample[loc][ci_ind]:stop_sample[loc][ci_ind]+step]).T\n",
        "  stop_sample[loc][ci_ind] = stop_sample[loc][ci_ind]+step\n",
        "\n",
        "  X_levels1 = []\n",
        "  for l1 in range(5):\n",
        "    for l2 in range(5):\n",
        "      len_condition = stop_sample[loc][l1*5+l2]\n",
        "      for _ in range(len_condition):\n",
        "        #print(\"inside loop \", l1,l2, l1*4+l2, len_condition)\n",
        "        X_levels1.append([1,l1,l2])\n",
        "\n",
        "  try:\n",
        "    reg_array_gp1[idx].fit(X_levels1, np.array(train_gp_LR1[idx]).reshape(-1,1) )\n",
        "  except:\n",
        "    print(\"Continue for loop\\n\")\n",
        "    print(np.shape(X_levels1), np.shape(train_gp_LR1[idx]))\n",
        "\n",
        "  batch_size2 = batch_size2 + step\n",
        "\n",
        "\n",
        "  reg_value_gp1 = round(reg_array_gp1[idx].intercept_[0],3)\n",
        "  reg_value_gp12 = round(reg_array_gp1[idx].coef_[0][1],3)\n",
        "  reg_value_gp13 = round(reg_array_gp1[idx].coef_[0][2],3)\n",
        "  #reg_value_gp14 = round(reg_array_gp1[idx].coef_[0][3],3)\n",
        "  #reg_value_gp15 = round(reg_array_gp1[idx].coef_[0][4],3)\n",
        "\n",
        "\n",
        "  count1[idx] += 1\n",
        "  count2[idx] += 1\n",
        "\n",
        "  train_X1 = torch.cat([train_X1,torch.tensor(new_pnt)]).double()\n",
        "  train_Y = torch.cat([train_Y,torch.tensor([[reg_value_gp1,reg_value_gp12,reg_value_gp13,]])]).double()\n",
        "\n",
        "  with gpytorch.settings.cholesky_max_tries(2000000):\n",
        "    gp = SingleTaskGP(train_X1, train_Y,\n",
        "                      #input_transform=Normalize(d=train_X1.shape[-1]),\n",
        "                      #outcome_transform=Standardize(m=num_par),\n",
        "                      #outcome_transform=None,\n",
        "                      covar_module=covar_module\n",
        "                      ).double()\n",
        "\n",
        "    #gp.train()\n",
        "    gp_mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
        "\n",
        "    #fit_gpytorch_mll(mll=gp_mll)\n",
        "\n",
        "    val_lr = gp.posterior(torch.Tensor(newls),).mean.tolist()\n",
        "    val_lr1 = [val_lr[i][0] for i in range(len(val_lr))]\n",
        "    val_lr2 = [val_lr[i][1] for i in range(len(val_lr))]\n",
        "    val_lr3 = [val_lr[i][2] for i in range(len(val_lr))]\n",
        "    #val_lr4 = [val_lr[i][3] for i in range(len(val_lr))]\n",
        "    #val_lr5 = [val_lr[i][4] for i in range(len(val_lr))]\n",
        "\n",
        "    total_value = len(map_df['NAME'].tolist())\n",
        "\n",
        "    mse_val_intercept = (sum(par1 -np.array(val_lr1) )**2) /total_value\n",
        "    mse_val_covariate = (sum(par2 - np.array(val_lr2) )**2) /total_value\n",
        "    mse_val_covariate2 = (sum(par3 - np.array(val_lr3) )**2) /total_value\n",
        "    #mse_val_covariate3 = (sum(par4 - np.array(val_lr4) )**2) /total_value\n",
        "\n",
        "    print(\"\\nIteration:\", iteration,\"/\",N_BATCH)\n",
        "    print(new_pnt,int(new_value),map_df['NAME'].loc[idx], reg_value_gp1,\n",
        "          #new_pnt2,int(new_value2),map_df['NAME'].loc[idx2], reg_value_gp2,\n",
        "          #'\\n',new_pnt3,int(new_value3),map_df['NAME'].loc[idx3], reg_value_gp3,\n",
        "          #new_pnt4,int(new_value4),map_df['NAME'].loc[idx4], reg_value_gp4,\n",
        "          '\\n',\"MSE:\",mse_val_intercept, mse_val_covariate, mse_val_covariate2, #mse_val_covariate3,\n",
        "          '\\n',gp.posterior(torch.Tensor(new_pnt)).mean.detach().numpy().flatten(),\n",
        "         # gp2.posterior(torch.Tensor(new_pnt2)).mean.detach().numpy().flatten(),\n",
        "         # '\\n', \"BGE:\", bge_value, bge_value2\n",
        "          )\n",
        "\n",
        "    loc=0\n",
        "    create_value = torch.Tensor([[map_df['geometry'].loc[loc].centroid.x, map_df['geometry'].loc[loc].centroid.y,\n",
        "                                  #income[loc],\n",
        "                                  pop_den[loc], #black_p[loc], #native_p[loc]\n",
        "                                    ]])\n",
        "    #create_value = torch.Tensor([[map_df['geometry'].loc[loc].centroid.x, map_df['geometry'].loc[loc].centroid.y]])\n",
        "    print(\"Allegheny County intercept:\",gp.posterior(create_value).mean.detach().numpy().flatten()\n",
        "  ,#gp2.posterior(create_value).mean.detach().numpy().flatten(),\n",
        "      )\n",
        "\n",
        "    loc=11\n",
        "    create_value = torch.Tensor([[map_df['geometry'].loc[loc].centroid.x, map_df['geometry'].loc[loc].centroid.y,\n",
        "                                  #income[loc],\n",
        "                                  pop_den[loc], #black_p[loc], #native_p[loc]\n",
        "                                    ]])\n",
        "    print(\"Cameron County intercept:\",gp.posterior(create_value).mean.detach().numpy().flatten()\n",
        "  ,#gp2.posterior(create_value).mean.detach().numpy().flatten(),\n",
        "    )\n",
        "\n",
        "  lr1 = np.array(val_lr1)\n",
        "  lr2 = np.array(val_lr2)\n",
        "  lr3 = np.array(val_lr3)\n",
        "  #lr4 = np.array(val_lr4)\n",
        "  #lr5 = np.array(val_lr5)\n",
        "  gp_value.append(sum((df_avg[0]- lr1 )**2))\n",
        "  gp_value2.append(sum((df_avg[1]- (lr1+lr2))**2 ))\n",
        "\n",
        "  gp_rate.append(sum((df_avg[0]/np.array(pop) - lr1/np.array(pop) )**2)/total_value)\n",
        "\n",
        "  val = []\n",
        "  val2= []\n",
        "  val3 = []\n",
        "  pop_100 = [100000 for i in range(total_value)]\n",
        "  for x in range(5):\n",
        "    for y in range(5):\n",
        "      val.append(sum((df_avg[x*5+y] - (lr1+lr2*x+lr3*y) )**2)/total_value)\n",
        "      val2.append(sum(( (df_avg[x*5+y]/np.array(pop)*np.array(pop_100)) - (np.array(lr1+lr2*x+lr3*y)/np.array(pop)*np.array(pop_100)) )**2)/total_value)\n",
        "      val3.append(sum( abs( (df_avg[x*5+y]/np.array(pop)*np.array(pop_100)) -\n",
        "        (np.array(lr1+lr2*x+lr3*y)/np.array(pop)*np.array(pop_100))) /(df_avg[x*5+y]/np.array(pop)*np.array(pop_100)))/total_value)\n",
        "  gp_rate2.append(sum(val)/25)\n",
        "  gp_rate3.append(sum(val2)/25)\n",
        "  gp_rate4.append(sum(val3)/25)\n",
        "\n",
        "  num_samples = [list(i.flatten()) for i in train_gp_LR1]\n",
        "  ################################################################################################\n",
        "  sample_post = []\n",
        "  for _ in range(100):\n",
        "    sample_post.append(gp.posterior(torch.Tensor(newls)).sample().tolist()[0])\n",
        "\n",
        "  samples_post = np.array(sample_post)\n",
        "  widest_indices = find_widest_ci_conditions(samples_post, num_levels1=5, num_levels2=5)\n",
        "  ################################################################################################\n",
        "  sum_num_samples = sum(num_samples, [])\n",
        "  print(np.shape(sum_num_samples), np.shape(X_levels1), np.shape(train_gp_LR1[idx]), gp_rate2[iteration-1],\n",
        "        gp_rate3[iteration-1], gp_rate4[iteration-1])\n",
        "  sample_complexity.append(np.shape(sum_num_samples)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUBJ1D9RoFc0"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri9iE85ZAAkw"
      },
      "outputs": [],
      "source": [
        "df_true = pd.DataFrame(columns=['Name','intercept','coef1','coef2','LR MSE rate','GPR MSE rate','GPR MSE count'])\n",
        "df_true['Name'] = map_df['NAME'].tolist()\n",
        "df_true['intercept'] = (lr1_true-lr1)**2\n",
        "df_true['coef1'] = (lr2_true-lr2)**2\n",
        "df_true['coef2'] = (lr3_true-lr3)**2\n",
        "\n",
        "lr_true = []\n",
        "gpr_rate = []\n",
        "gpr_count = []\n",
        "#for k in range(len(map_df)):\n",
        "val = []\n",
        "val2 = []\n",
        "val3 = []\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    val.append( (df[i*5+j].mean().tolist()/np.array(pop)*np.array(pop_100)- ((lr1_true+lr2_true*i+lr3_true*j)/np.array(pop)*np.array(pop_100) ) )**2 )\n",
        "    val2.append( ((df[i*5+j].mean().tolist()/np.array(pop)*np.array(pop_100))- ((lr1+lr2*i+lr3*j)/np.array(pop)*np.array(pop_100)) )**2 )\n",
        "    val3.append( (df[i*5+j].mean().tolist() - (lr1+lr2*i+lr3*j) )**2 )\n",
        "\n",
        "\n",
        "  #lr_true.append(sum(val)/25)\n",
        "  #gpr_rate.append(sum(val2)/25)\n",
        "  #gpr_count.append(sum(val3)/25)\n",
        "df_true['LR MSE rate'] = np.average(val,axis=0)\n",
        "df_true['GPR MSE rate'] = np.average(val2,axis=0)\n",
        "df_true['GPR MSE count'] = np.average(val3,axis=0)\n",
        "df_true['mean death rate'] = df_avg_all/np.array(pop)*np.array(pop_100)\n",
        "df_true['mean death count'] = df_avg_all\n",
        "df_true.to_csv('df_true.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare"
      ],
      "metadata": {
        "id": "P44eAnIjJBnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASyfBJhbJ9fr"
      },
      "outputs": [],
      "source": [
        "compare = pd.DataFrame(data=zip(sample_complexity,gp_rate3, gp_rate4), columns=['sample','mse','rel_error'])\n",
        "compare.to_csv(\"pa25_seqdes.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj8u6Hs6evCw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- file paths ----------\n",
        "#file_rand = \"pa25_2.csv\"          # one-stage sequential design\n",
        "file_rand = \"pa25_seqdes.csv\"\n",
        "file_seq  = \"pa25_seqdes2.csv\"   # two-stage sequential design\n",
        "\n",
        "# ---------- read & transform ----------\n",
        "rand_df = pd.read_csv(file_rand)\n",
        "seq_df  = pd.read_csv(file_seq)\n",
        "\n",
        "for df_plot in (rand_df, seq_df):\n",
        "    df_plot[\"rel_error_pct\"] = df_plot[\"rel_error\"] * 100.0\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "m=60\n",
        "# --------------------------------------------------------------------\n",
        "# 1) MSE plot\n",
        "# --------------------------------------------------------------------\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(rand_df[\"sample\"].loc[:m], rand_df[\"mse\"].loc[:m], label=\"one-stage sequential design\")\n",
        "plt.plot(seq_df[\"sample\"],  seq_df[\"mse\"],  label=\"two-stage sequential design\")\n",
        "plt.xlabel(\"Number of samples\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"Mean-squared error vs. sample size\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2) Relative-error plot\n",
        "# --------------------------------------------------------------------\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(rand_df[\"sample\"].loc[:m], rand_df[\"rel_error_pct\"].loc[:m], label=\"one-stage sequential design\")\n",
        "plt.plot(seq_df[\"sample\"],  seq_df[\"rel_error_pct\"],  label=\"two-stage sequential design\")\n",
        "plt.xlabel(\"Number of samples\")\n",
        "plt.ylabel(\"Relative error (%)\")\n",
        "#plt.title(\"Relative error vs. sample size\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}